# -*- coding: utf-8 -*-
"""Copy of JSC270 Assignment 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Fredooooooo/JSC270_Assg4/blob/main/Copy_of_JSC270_Assignment_4.ipynb
"""

api_key = 'RnHS3Y3lkrv9pzkHVFcVYVArH'
api_secret_key = 'xLMxTxhEHluPlcsPgb2rur07hBR4qSuzGdIVhJg0hpEiR1nTBm'
access_key = '1376070608950267904-AlJkcNLLxsAByp9NmFU854synmuzq5'
access_secret = 'dtH18I0v8Ob44BAUXhrJB9TNvnFfQykBafXY4fl4JtdJB'

import re
import numpy as np
import pandas as pd 
import io
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import *
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, roc_curve
import matplotlib.pyplot as plt

"""## Section 1

**Load datasets**
"""

from google.colab import files
data_to_load = files.upload()

train_data = pd.read_csv(io.BytesIO(data_to_load['covid-tweets-train.csv']), encoding = 'latin1') 
print(train_data.head(5))

data_to_load = files.upload()

test_data = pd.read_csv(io.BytesIO(data_to_load['covid-tweets-test.csv']), encoding = 'latin1') 
print(test_data.head(5))

def tokenization(data, col):
  nltk.download('punkt')

  # Create a new column in our DF that contains token lists instead of raw text
  data['tokens'] = data[col].apply(nltk.word_tokenize)

def remove_url(data, col, n):
  lst = []
  for i in range(0,n):
    lst.append(re.sub(r'http\S+', '', data[col][i]))
  data['url_removed'] = lst

def evaluate(X_test, y_test, model):
  y_preds = model.predict(X_test)

  print('Test accuracy with our model:',accuracy_score(y_test,y_preds))

"""**(A)**"""

nega = 0
neut = 0
posi = 0
num_train = len(train_data)
num_test = len(test_data)
for sentiment in train_data['Sentiment']:
  if sentiment == '0':
    nega += 1
  elif sentiment == '1':
    neut += 1
  elif sentiment == '2':
    posi += 1
print('the proportion of negative sentiment: ', nega/num_train)
print('the proportion of neutral sentiment: ', neut/num_train)
print('the proportion of positive sentiment: ', posi/num_train)

"""**(B)**"""

tokenization(train_data, 'OriginalTweet')
tokenization(test_data, 'OriginalTweet')

print(train_data['tokens'].head(5))
print(test_data['tokens'].head(5))

"""**(C)**"""

remove_url(train_data, 'OriginalTweet', num_train)
tokenization(train_data, 'url_removed')
train_data

remove_url(test_data, 'OriginalTweet', num_test)
tokenization(test_data, 'url_removed')
test_data

"""(D)"""

##### Convert tokens into lowercase ####
lowercase_tokens = []
# Create a list of lists with what we want
for row in train_data['tokens']:
  lowercase_tokens.append([t.lower() for t in row])
# add the new info to our df
train_data['lowercase_tokens'] = lowercase_tokens

print(train_data['lowercase_tokens'].head(5))

##### Let's remove punctuation #####

# Note we've been keeping different columns for different steps (not necessary)
list(train_data)

# Same process as before
tokens_no_punct = []
# Create a list of lists with what we want
for row in train_data['lowercase_tokens']:
  tokens_no_punct.append([re.sub('[^\w\s]','', t) for t in row])
# add the new info to our df
train_data['tokens_no_punct'] = tokens_no_punct

print(train_data['tokens_no_punct'].head(5))

##### Convert tokens into lowercase ####
lowercase_tokens = []
# Create a list of lists with what we want
for row in test_data['tokens']:
  lowercase_tokens.append([t.lower() for t in row])
# add the new info to our df
test_data['lowercase_tokens'] = lowercase_tokens

print(test_data['lowercase_tokens'].head(5))

##### Let's remove punctuation #####

# Note we've been keeping different columns for different steps (not necessary)
list(test_data)

# Same process as before
tokens_no_punct = []
# Create a list of lists with what we want
for row in test_data['lowercase_tokens']:
  tokens_no_punct.append([re.sub('[^\w\s]','', t) for t in row])
# add the new info to our df
test_data['tokens_no_punct'] = tokens_no_punct

print(test_data['tokens_no_punct'].head(5))

"""(E)"""

#### Stemming tokens ####
from nltk.stem.porter import *
stemmer = PorterStemmer()
stemmed_tokens = []
for row in train_data['tokens_no_punct']:
  stemmed_tokens.append([stemmer.stem(t) for t in row])

train_data['stemmed_tokens'] = stemmed_tokens
print('After stemming:\n', train_data['stemmed_tokens'].head(3))

#### Stemming tokens ####
from nltk.stem.porter import *
stemmer = PorterStemmer()
stemmed_tokens = []
for row in test_data['tokens_no_punct']:
  stemmed_tokens.append([stemmer.stem(t) for t in row])

test_data['stemmed_tokens'] = stemmed_tokens
print('After stemming:\n', test_data['stemmed_tokens'].head(3))

# Let's use the TFIDF counts for modelling
X_train = tfs_train.toarray()
X_test = tfs_test.toarray()
y_train = y_train.tolist()
y_test = y_test.tolist()
# Let's fit the Naive Bayes model to our training data
nb = MultinomialNB()
# Fit model to training data
nb.fit(X_train, y_train)
# Predict on test data
y_train_preds = nb.predict(X_train)

# Get fitted values from test set
y_test_preds = nb.predict(X_test)

# Print train and test errors
train_acc = accuracy_score(y_train, y_train_preds)
test_acc = accuracy_score(y_test, y_test_preds)

print('Train Accuracy: ',train_acc)
print('Test Accuracy: ',test_acc)from nltk.stem.porter import *
stemmer = PorterStemmer()
stemmed_tokens = []
for row in df['tokens_no_sw']:
  stemmed_tokens.append([stemmer.stem(t) for t in row])

df['stemmed_tokens'] = stemmed_tokens
print('Before stemming:\n', df['tokens_no_sw'].head(3))
print('After stemming:\n', df['stemmed_tokens'].head(3))

# Drop some intermediate columns
list(train_data)
train_data = train_data.drop(['tokens','lowercase_tokens', 'tokens_no_punct','url_removed'],axis=1)

print('After removal:\n', list(train_data))

no_blanks = []
for row in train_data['stemmed_tokens']:
  no_blanks.append([t for t in row if t != ''])
train_data['tokens'] = no_blanks

print(train_data['tokens'].head(5))

# drop the last intermediate column
train_data = train_data.drop(['stemmed_tokens'], axis = 1)

# Now we should have only the label, original text, and cleaned token lists
print('Current Columns:\n',list(train_data))

# Drop some intermediate columns
test_data = test_data.drop(['tokens','lowercase_tokens', 'tokens_no_punct','url_removed'],axis=1)

print('After removal:\n', list(test_data))

no_blanks = []
for row in test_data['stemmed_tokens']:
  no_blanks.append([t for t in row if t != ''])
test_data['tokens'] = no_blanks


# drop the last intermediate column
test_data = test_data.drop(['stemmed_tokens'], axis = 1)

# Now we should have only the label, original text, and cleaned token lists
print('Current Columns:\n',list(test_data))

"""(F)"""

##### Time to remove Stopwords #####

from nltk.corpus import stopwords
nltk.download('stopwords')
# print the top 100 most popular english words
sw = stopwords.words('english')[:100]

# Now let's remove them
tokens_no_sw = []
for row in train_data['tokens']:
  tokens_no_sw.append([w for w in row if w not in sw])
# Add column to df
train_data['tokens'] = tokens_no_sw

# Print some examples
print(train_data['tokens'].tail(5))

# Now let's remove them
tokens_no_sw = []
for row in test_data['tokens']:
  tokens_no_sw.append([w for w in row if w not in sw])
# Add column to df
test_data['tokens'] = tokens_no_sw

# Print some examples
print(test_data['tokens'].tail(5))

"""（G）"""

for i in range(0, len(train_data['Sentiment'])):
  if type(train_data['Sentiment'][i]) == str:
    if train_data['Sentiment'][i].isnumeric():
      train_data['Sentiment'][i] = int(train_data['Sentiment'][i])
    else:
      train_data['Sentiment'][i] = 1
for i in range(0, len(test_data['Sentiment'])):
  if type(test_data['Sentiment'][i]) == str:
    if test_data['Sentiment'][i].isnumeric():
      test_data['Sentiment'][i] = int(test_data['Sentiment'][i])
    else:
      test_data['Sentiment'][i] = 1

for i in range(0, len(test_data['Sentiment'])):
  if np.isnan(int(test_data['Sentiment'][i])):
    test_data['Sentiment'][i] = 1
  else:
    test_data['Sentiment'][i] = int(test_data['Sentiment'][i])

X_train, y_train = train_data['tokens'].to_numpy(), train_data['Sentiment'].to_numpy()
X_test, y_test = test_data['tokens'].to_numpy(), test_data['Sentiment'].to_numpy()
def override_fcn(doc):
  # We expect a list of tokens as input
  return doc

# Count Vectorizer
count_vec = CountVectorizer(
    analyzer='word',
    tokenizer= override_fcn,
    preprocessor= override_fcn,
    token_pattern= None,
    max_features = 1000)

# Remember this output is a Scipy Sparse Array
counts_train = count_vec.fit_transform(X_train)
counts_test = count_vec.fit_transform(X_test)

X_train = counts_train.toarray()
X_test = counts_test.toarray()

for i in range(0, len(y_train)):
  if np.isnan(y_train[i]):
    y_train[i] = 1


y_test = y_test.tolist()

y_train = y_train.tolist()

print(len(count_vec.vocabulary_))

"""**(H)**"""

nb = MultinomialNB()
# Fit model to training data
nb.fit(X_train, y_train)
# Predict on test data
y_train_preds = nb.predict(X_train)

# Get fitted values from test set
y_test_preds = nb.predict(X_test)

# Print train and test errors
train_acc = accuracy_score(y_train, y_train_preds)
test_acc = accuracy_score(y_test, y_test_preds)

print('Train Accuracy: ',train_acc)
print('Test Accuracy: ',test_acc)

for lst in test_data['tokens']:
  text.extend(lst)
text = ' '.join(text)
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

for lst in train_data['tokens']:
  text.extend(lst)
text = ' '.join(text)
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""**(I)**

No, because ROC curve is not suitable for labels involving multiple classes

**(J)**
"""

X_train, y_train = train_data['tokens'].to_numpy(), train_data['Sentiment'].to_numpy()
X_test, y_test = test_data['tokens'].to_numpy(), test_data['Sentiment'].to_numpy()
def override_fcn(doc):
  # We expect a list of tokens as input
  return doc

# Count Vectorizer
count_vec = CountVectorizer(
    analyzer='word',
    tokenizer= override_fcn,
    preprocessor= override_fcn,
    token_pattern= None,
    max_features = 1000)

# Remember this output is a Scipy Sparse Array
counts_train = count_vec.fit_transform(X_train)
counts_test = count_vec.fit_transform(X_test)
#### TF-IDF Vectorize ####

# Note that smoothing is done by default
tfidf = TfidfTransformer()

tfs_train = tfidf.fit_transform(counts_train)
tfs_test = tfidf.fit_transform(counts_test)

# Let's use the TFIDF counts for modelling
X_train = tfs_train.toarray()
X_test = tfs_test.toarray()
print(X_test)
y_train = y_train.tolist()
y_test = y_test.tolist()
# Let's fit the Naive Bayes model to our training data
nb = MultinomialNB()
# Fit model to training data
nb.fit(X_train, y_train)
# Predict on test data
y_train_preds = nb.predict(X_train)

# Get fitted values from test set
y_test_preds = nb.predict(X_test)

# Print train and test errors
train_acc = accuracy_score(y_train, y_train_preds)
test_acc = accuracy_score(y_test, y_test_preds)

print('Train Accuracy: ',train_acc)
print('Test Accuracy: ',test_acc)

"""**(K)**"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
lem_tokens = []
for row in train_data['tokens']:
  lem_tokens.append([lemmatizer.lemmatize(t) for t in row])

train_data['lem_tokens'] = lem_tokens

# Print results
print(train_data['lem_tokens'].head(5))
## For test data
lemmatizer = WordNetLemmatizer()
lem_tokens = []
for row in test_data['tokens']:
  lem_tokens.append([lemmatizer.lemmatize(t) for t in row])

test_data['lem_tokens'] = lem_tokens

# Print results
print(test_data['lem_tokens'].head(5))

tokens_no_sw = []
for row in train_data['tokens']:
  tokens_no_sw.append([w for w in row if w not in sw])
# Add column to df
train_data['tokens'] = tokens_no_sw

tokens_no_sw = []
for row in test_data['tokens']:
  tokens_no_sw.append([w for w in row if w not in sw])
# Add column to df
test_data['tokens'] = tokens_no_sw

X_train, y_train = train_data['tokens'].to_numpy(), train_data['Sentiment'].to_numpy()
X_test, y_test = test_data['tokens'].to_numpy(), test_data['Sentiment'].to_numpy()
def override_fcn(doc):
  # We expect a list of tokens as input
  return doc

# Count Vectorizer
count_vec = CountVectorizer(
    analyzer='word',
    tokenizer= override_fcn,
    preprocessor= override_fcn,
    token_pattern= None,
    max_features = 1000)

# Remember this output is a Scipy Sparse Array
counts_train = count_vec.fit_transform(X_train)
counts_test = count_vec.fit_transform(X_test)
#### TF-IDF Vectorize ####

# Note that smoothing is done by default
tfidf = TfidfTransformer()

tfs_train = tfidf.fit_transform(counts_train)
tfs_test = tfidf.fit_transform(counts_test)

# Let's use the TFIDF counts for modelling
X_train = tfs_train.toarray()
X_test = tfs_test.toarray()
y_train = y_train.tolist()
y_test = y_test.tolist()
# Let's fit the Naive Bayes model to our training data
nb = MultinomialNB()
# Fit model to training data
nb.fit(X_train, y_train)
# Predict on test data
y_train_preds = nb.predict(X_train)

# Get fitted values from test set
y_test_preds = nb.predict(X_test)

# Print train and test errors
train_acc = accuracy_score(y_train, y_train_preds)
test_acc = accuracy_score(y_test, y_test_preds)

print('Train Accuracy: ',train_acc)
print('Test Accuracy: ',test_acc)

"""## Section 2"""

import tweepy as tw
# Use credentials to authorize access
auth = tw.OAuthHandler(api_key, api_secret_key)
auth.set_access_token(access_key, access_secret)
api = tw.API(auth, wait_on_rate_limit=True)
search_words = ' -filter:retweets'
date_since = "2021-04-02"

#Collect tweets (here, I get only 20)
tweets = tw.Cursor(api.search,
              q=search_words,
              lang="en",
              since=date_since).items(1000)

# The result is an iterable
a = []
b = []
for tweet in tweets:
  a.append(tweet.text)
  b.append(tweet.user.followers_count)

import pandas as pd
import re

# tweet_list = [tweet.text for tweet in tweets]
df = pd.DataFrame(a, columns = ['tweet'])
df['followers'] = b
print(df.count())

handle_regex = '@[A-Za-z|0-9|_]+'

d = []
for row in df['tweet']:
  d.append(re.sub(handle_regex,'', row))

df['tweet'] = d
df.head()

c = []
for x in df['followers']:
  if 0 <= x <= 100:
    c.append(0)
  elif 100 < x <= 10000:
    c.append(1)
  else:
    c.append(2)
df['labels'] = c
df.head()

low_proportion = df['labels'].tolist().count(0)
medium_proportion = df['labels'].tolist().count(1)
high_proportion = df['labels'].tolist().count(2)
print(low_proportion, medium_proportion, high_proportion)
print(df.followers.mean)

import nltk
# Download the tokenizer
nltk.download('punkt')

# Create a new column in our DF that contains token lists instead of raw text
df['tokens'] = df['tweet'].apply(nltk.word_tokenize)

print(df['tokens'].head(5))

##### Convert tokens into lowercase ####
lowercase_tokens = []
# Create a list of lists with what we want
for row in df['tokens']:
  lowercase_tokens.append([t.lower() for t in row])
# add the new info to our df
df['lowercase_tokens'] = lowercase_tokens

print(df['lowercase_tokens'].head(5))

##### Let's remove punctuation #####

# Note we've been keeping different columns for different steps (not necessary)
list(df)

# Same process as before
tokens_no_punct = []
# Create a list of lists with what we want
for row in df['lowercase_tokens']:
  tokens_no_punct.append([re.sub('[^\w\s]','', t) for t in row])
# add the new info to our df
df['tokens_no_punct'] = tokens_no_punct

print(df['tokens_no_punct'].head(5))

##### Time to remove Stopwords #####
import numpy as np
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')[:75]
tokens_no_sw = []
for row in df['tokens_no_punct']:
  tokens_no_sw.append([w for w in row if w not in sw])
# Add column to df
df['tokens_no_sw'] = tokens_no_sw

# Print some examples
print(df['tokens_no_sw'].head(5))

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
lem_tokens = []
for row in df['tokens_no_sw']:
  lem_tokens.append([lemmatizer.lemmatize(t) for t in row])

df['lem_tokens'] = lem_tokens

# Print results
print(df['lem_tokens'].head(5))

# Drop some intermediate columns
list(df)
df = df.drop(['tokens','lowercase_tokens', 'tokens_no_punct','tokens_no_sw','stemmed_tokens'], axis = 1)

print('After removal:\n', list(df))

# Remove blank tokens
no_blanks = []
for row in df['lem_tokens']:
  no_blanks.append([t for t in row if t != ''])
df['tokens'] = no_blanks

print(df['tokens'].head(5))

# drop the last intermediate column
df = df.drop(['lem_tokens'], axis = 1)

# Now we should have only the label, original text, and cleaned token lists
print('Current Columns:\n',list(df))

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
popular = df['labels'] > 1
df_pop = df[popular]
text = []
for lst in df_pop['tokens']:
  text.extend(lst)
text = ' '.join(text)
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
popular = df['labels'] == 1
df_less_pop = df[popular]
text = []
for lst in df_less_pop['tokens']:
  text.extend(lst)
text = ' '.join(text)
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
popular = df['labels'] < 1
df_lst_pop = df[popular]
text = []
for lst in df_lst_pop['tokens']:
  text.extend(lst)
text = ' '.join(text)
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df_pop.head()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import train_test_split

# Separate labels from features, converting to numpy arrays
X, y = df['tokens'].to_numpy(), df['labels'].to_numpy()


def override_fcn(doc):
  # We expect a list of tokens as input
  return doc

# Count Vectorizer
count_vec = CountVectorizer(
    analyzer='word',
    tokenizer= override_fcn,
    preprocessor= override_fcn,
    token_pattern= None)

# Remember this output is a Scipy Sparse Array
counts = count_vec.fit_transform(X)
print(counts.toarray())

# Print the names of each of the features (1000 total))
print(count_vec.get_feature_names())
# Print this mapping as dictionary
print(count_vec.vocabulary_)

#### TF-IDF Vectorize ####

# Note that smoothing is done by default
tfidf = TfidfTransformer()

tfs = tfidf.fit_transform(counts)

# Let's use the TFIDF counts for modelling
X = tfs.toarray()

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# First three rows of training features and labels

# Commented out IPython magic to ensure Python compatibility.
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, roc_curve
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
# %matplotlib inline

# Let's fit the Naive Bayes model to our training data
nb = MultinomialNB()
# Fit model to training data
nb.fit(X_train, y_train)
# Predict on test data
y_preds = nb.predict(X_test)

print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test,y_preds))

# fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label = 1)

# plt.plot(fpr,tpr)
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Receiver Operator Characteristic (ROC) Curve for RF')
# plt.show()

labels = [0,1,2]
print(classification_report(y_test, y_preds, labels)) #classification report from sklearn
cnf_matrix = confusion_matrix(y_test, y_preds, labels=labels)
plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid
threshold = cnf_matrix.max() / 2 #threshold to define text color
for i in range(cnf_matrix.shape[0]): #print text in grid
    for j in range(cnf_matrix.shape[1]): 
        plt.text(j, i, cnf_matrix[i,j], color="w" if cnf_matrix[i,j] > threshold else 'black')
tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes
plt.xticks(tick_marks, labels, rotation=45)
plt.yticks(tick_marks, labels)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.colorbar()
plt.tight_layout()

from sklearn.ensemble import RandomForestClassifier

# Instantiate the model object
rfclf = RandomForestClassifier(n_estimators = 50, max_depth = 6, random_state = 10, max_features='log2')

# Fit the model to the data
rfclf.fit(X_train, y_train)

# Get fitted values from test set
y_test_preds_rf = rfclf.predict(X_test)

test_acc = accuracy_score(y_test, y_test_preds_rf)

print('Test Accuracy: ',test_acc)

"""**export to csv**"""

df.to_csv(r'C:\Users\HP\Documents\output.csv', index=False)